---
title: "Homework 6" 
author: "Max Ryoo (hr2ee)"
fontsize: 12pt
geometry: margin=1in
output: pdf_document

---
library(ggplot2)
library(car)
library(gcookbook)
library(MASS)
library(Hmisc)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, highlight=FALSE)
```
## Problem 1
### A
```{r}
set.seed(12181998)
library(ggplot2)
library(car)
library(gcookbook)
library(MASS)
library(Hmisc)
setwd("/Users/maxryoo/Documents/Fall 2018/STAT3080/HW6")
crash <- read.csv("fatal accidents.csv")
states <- as.vector(unique(crash[1]))
statenames <- states[[1]]
state.list <- lapply(1:length(statenames)
                     , function(x) crash[crash$State == statenames[x],])
```
First I read in the csv file of fatal accidents. I then found the names of the different states and stored it into a states vector. I then stored that into a statenames vector. I then used the lapply function to loop through the statenmaes vector and use the function of finding which entries coreespond to the statename and stored that data frame in indicies of the state.list list.I will not print this due to it being too long

### B
```{r}
firstfew <- lapply(1:length(statenames), function(x) state.list[[x]][1:3,] )
print(firstfew)
```
I applied the function of printing the first three indicies of index x, which has a value between 1 and length of the vector oc statenames. then i printed the firstfew vector holding these dataframes.

### C
```{r}
numvhe <- lapply(1:length(statenames), 
                 function(x) as.data.frame(
                   table(state.list[[x]]$Vehicle.count)))
addstate<-lapply(1:length(statenames), 
                 function(x) cbind(
                   numvhe[[x]], State = statenames[x]))
vehicles_bystate <- lapply(addstate, 
                           setNames, 
                           nm = c("Number of vehicles involved",
                                  "Frequency", "State"))
print(vehicles_bystate)
```
I first made numvhe list which as the dataframe of number of vehicles involved as well as the frequency. I then made another list addstate, which adds the corressponding states for each item in the list. I then added the column names for more description and stored into a list called vehicles_bystate, which i printed.

### D 
```{r}
numaccstate <- table(crash$State,
                     crash$Vehicle.count)
print(numaccstate)
```
For D I simply used the table function to show a table of accidents with certain vehicle count for each state. Each table entry is a frequency. For example 16 instances of 1 vehicle crash for District of Columbia. 

### E
```{r}
totalacc<-lapply(1:length(statenames),
                 function(x) 
                   sum(as.vector(
                     numaccstate[x,])) )
print(totalacc)
```
Doing numaccstate[x,] will give me the row of the table (row 1 if x = 1). I then took the sum of the vector form of that result. I used the lapply function to do it for all the different states, which I stored the list into a totalacc.

### F
```{r}
percent <- sapply(1:length(statenames),
                  function(x) 
                    round(numaccstate[x, ]/totalacc[[x]],
                          digits = 1))
percentage <- t(percent)
tablep <- as.table(percentage)
row.names(tablep) <- statenames
print(tablep)
```
I used sapply to divide each entry in numaccstate by the total accidents. The numacccstate is the table used in previous questions and totalacc is the list containing the total number of accidents. I then rounded the answers to the 0.1. I stored it into percentage and made it into a table using as.table(). I then proceeded to give names for the rows (which were the steates names)

### G
For all states there were more accidents for 1 vehicles involved. The more vehicles involved the lesser the counts of accidents for all states

### H
```{r}
numvehday <- lapply(1:length(statenames),
                    function(x) 
                      crash[crash$State == 
                              statenames[x],])
listtable <- lapply(1:length(numvehday), 
                    function(x) table(
                      numvehday[[x]]$Day.of.week,
                      numvehday[[x]]$Vehicle.count) )
print(listtable)
```
I first made the list called nuvehday. This list contains the dataframe for each state. List of dataframes. I then used the lapply function to make tables for each state, a table of Days of week and vehicle count and their frequencies for each state. I put it into listtable. This is a list of tables. I then printed the result. 

### I 
There seems to be more accidents in the weekend for some states, which is more probable since people tend to go out to new roads during the weekends to travel. Meanwhile the weekdays its the same regular route, which results in lesser accidents


## Problem 2
### A
```{r}
meanpop =62.9
sdpop = 13.3
samp <- as.vector(rnorm(25,
                        mean= meanpop, 
                        sd=sdpop))
print(samp)
```
I set manpop and sdpop as the parameters given. I then did a rnorm of 25 entries with the given mean and sd and saved it into samp as a vector.
### B
```{r}
s_mean <- mean(samp)
s_sd <- sd(samp)
p_val <- 2*pnorm(
  abs((s_mean-meanpop)/(sdpop/sqrt(25)))
                 , lower.tail=FALSE)
print(p_val<0.1)
```
I set s_mean as mean of the samp vectgor. I set s_sd as the sd of samp. I set the p_val with the pnorm with the equation for ztest. I then printed whether the p_val was lower than 0.1, which was false meaning fail to reject.
### C
```{r}
rep_sample <- replicate(10000, 
                        rnorm(25, 
                              mean=meanpop,
                              sd =sdpop))
rep_mean <- sapply(1:10000, function(x)
  mean(rep_sample[,x]))
rep_p <- sapply(1:10000, function(x) 
  2*pnorm(abs((rep_mean[x]-meanpop)/(sdpop/sqrt(25))), lower.tail=FALSE))
rejects <- as.vector(which(rep_p < 0.1))
proportion <- length(rejects)/10000
print(proportion)
```
Reg_sample contains rnorm of 25 entries being done 10,000 times. I then used sapply to this list to find the mean of the 10000 trials. I used the sapply function again to find the pnorm with the new mean that we found previously in rep_mean. I then found which entries have a p value less than 0.1 and stored it as a vector into rejects. I found the length of the rejects function and divided by 10000 to get the proportion. 

### D
Theoretically, this value should be 0.1 Since the upper and lower bound is each 0.05,  which together would be 0.1. The possibility of getting a value that rejects the null hypothesis is 0.05 for the upper and 0.05 for the lower, which together makes up a probability of 0.1. This is similar to the value calcualted by C, although this will change every time you run it it is close to 0.1

## References
1. <https://stackoverflow.com/questions/
10234734/converting-a-numeric-matrix-into-a-data-table-or-data-frame>
