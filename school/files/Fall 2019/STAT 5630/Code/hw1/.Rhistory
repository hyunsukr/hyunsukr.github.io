nA <- length(rpA)
nB <- length(rpB)
min(tsA,tsB)
tsA <- nA*nB + (nA*(nA+1)/2) - sums[1]
tsB <- nA*nB + (nB*(nB+1)/2) - sums[2]
min(tsA,tsB)
## Create two-way table for 114 individuals' hair and eye color
M <- matrix(c(38,14,11,51), nrow=2, ncol=2)
dimnames(M) <- list(Hair=c("Fair","Dark"), Eye=c("Blue","Brown"))
M
## Create a two-way table for 400 individuals' gender and education
L <- matrix(c(63,42,54,44,46,53,41,57), nrow=2, ncol=4)
dimnames(L) <- list(Gender=c("Female","Male"),
Education=c("HS","BA","MA","PhD"))
L
sum(L)
## Joint distribution in proportions
Lp <- prop.table(L)
sum(Lp)
## Conditional distributions of education levels
L.cond.edu <- prop.table(L,2)
L.cond.edu
apply(L.cond.edu,2,sum)
apply(L.cond.edu,2,sum)
L.cond.edu
## Joint distribution in proportions
Lp <- prop.table(L)
sum(Lp)
Lp
### IN CLAS ###
library(ggplot2)
k <- 10000
n <-7
ps <- 0.3
mu <- n * ps
ggplot(df, aes(x=sampes)) + geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") + stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
k <- 10000
n <-7
ps <- 0.3
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
ggplot(df, aes(x=sampes)) + geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") + stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
k <- 10000
n <-7
ps <- 0.3
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-28
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-49
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-70
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-28
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-49
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
k <- 10000
n <-49
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-70
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
## Conduct the test
phat <- c(0.117, 0.121)
n <- c(50000, 60000)
x <- phat*n
z2.prop <- prop.test(x, n, alternative="two.sided", correct=FALSE)
z2.prop
## Determine test statistic
comb.phat <- sum(x)/sum(n)
z <- diff(phat)/sqrt(comb.phat*(1-comb.phat)*(1/n[1]+1/n[2]))
z
## Determine p-value
p.value <- 2*(1-pnorm(z))
p.value
## Compare test statistic and p-value to output
sqrt(z2.prop$statistic)
z2.prop$p.value
z1.prop <- prop.test(x, n, alternative="one.sided", correct=FALSE)
z1.prop <- prop.test(x, n, alternative="less", correct=FALSE)
z1.prop$p.value
## Calculate the power
library(pwr)
pwr.chisq.test(w=0.3, N=400, df=3, sig.level=0.05)
## Determine the sample size required for power of 0.8
pwr.chisq.test(w=0.3, df=3, sig.level=0.05, power=0.8)
## Calculate the power
pwr.chisq.test(w=0.1, N=200, df=3, sig.level=0.05)
## Determine the sample size required for power of 0.8
pwr.chisq.test(w=0.1, df=3, sig.level=0.05, power=0.8)
## Calculate the power
pwr.chisq.test(w=0.1, N=200, df=3, sig.level=0.05)
## Determine the sample size required for power of 0.8
pwr.chisq.test(w=0.1, df=3, sig.level=0.05, power=0.8)
## Calculate the power
pwr.p.test(h=-0.2, n=100, sig.level=0.05, alternative="less")
## Determine the sample size required for power of 0.8
pwr.p.test(h=-0.2, sig.level=0.05, power=0.8, alternative="less")
#starting to look at pipelines
library(tidyverse)
salary_games_info <- read_csv('Lect2data.csv')
head(salary_games_info)
##############################################################
##############################################################
################### Ceci n'est pas une pipe ##################
##############################################################
##############################################################
salary_games_info %>% head()
salary_games_info %>% head() %>% tail(3)
tail(head(salary_games_info), 3)
salary_small_save <- salary_games_info %>% head() %>% tail(3)
##############################################################
##############################################################
######################## SUMMARIZE ###########################
##############################################################
##############################################################
salary_games_info %>%
summarize(count=n())
salary_games_info %>%
summarize(average_salary=mean(salary))
salary_games_info %>% summary()
salary_games_info %>%
summarize(average_salary=mean(salary)) %>% is_tibble()
salary_games_info %>% summary() %>% is_tibble()
##############################################################
##############################################################
########################## FILTER ############################
##############################################################
##############################################################
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000)
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000) %>% tail()
##############################################################
##############################################################
######################### ARRANGE ############################
##############################################################
##############################################################
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000) %>% arrange(desc(yearID))
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000) %>% arrange(desc(yearID), salary)
##############################################################
##############################################################
######################## GROUP_BY ############################
##############################################################
##############################################################
salary_games_info %>%
group_by(playerID) %>%
summarize(count=n())
salary_games_info %>%
group_by(playerID) %>%
summarize(count=n()) %>%
arrange(desc(count))
salary_games_info %>%
group_by(playerID) %>%
summarize(numYearsPlayed = length(unique(yearID)), count=n())
salary_games_info %>%
group_by(playerID) %>%
arrange(yearID) %>%
mutate(yearContinuity = yearID - lag(yearID)) %>%
filter(yearID>1985)
##############################################################
##############################################################
######################### MUTATE #############################
##############################################################
##############################################################
salary_games_info %>% mutate(sqrt_games = sqrt(G))
salary_games_info %>% mutate(atbats_per_game = AB/G)
#starting to look at pipelines
library(tidyverse)
package.install(tidyverse)
install.packages(tidyverse)
install.packages(tidyverse)
install.package(tidyverse)
# goal is to get data into tidy format.
# often data is not tidy, so you have to do some work
# what is tidy data?
#     each variable has its own column
#     each observation gets its own row, and not multiple rows
#     one datapoint per cell
library(tidyverse)
data <- tibble(group = sample(c('red', 'green', 'blue'), 100, replace=TRUE),
`2011`=runif(100), `2012`=rnorm(100), `2013`=1:100)
data.tidy <- data %>% gather(key="year", value="measurement", -group)
data.tidy <- data %>% gather(key="year", value="measurement", -group) %>%
mutate(year=as.integer(year))
data <- tibble(observation=rep(1:10, each=3),
meas_var=rep(c('length', 'width', 'height'), times=10),
measurement=runif(30))
data.tidy <- data %>% spread(key="meas_var", value="measurement")
data.tidy
data <- tibble(group = sample(c('red', 'green', 'blue'), 100, replace=TRUE),
`2011`=runif(100), `2012`=rnorm(100), `2013`=1:100)
data
data.tidy <- data %>% gather(key="year", value="measurement", -group)
data.tidy
data.tidy <- data %>% gather(key="year", value="measurement", -group) %>%
mutate(year=as.integer(year))
data.tidy
View(data.tidy)
data <- tibble(observation=rep(1:10, each=3),
meas_var=rep(c('length', 'width', 'height'), times=10),
measurement=runif(30))
view(data)
View(data)
View(data)
data.tidy
View(data)
View(data.tidy)
data.tidy <- data %>% spread(key="meas_var", value="measurement")
data.tidy <- data %>% spread(key="meas_var", value="measurement")
View(data.tidy)
View(data)
library(tidyverse)
flights <- read_csv('flights_jan.csv')
flights %>% mutate("Weekend" = ((flights["DAY_OF_WEEK"] > 5) = 1)) %>% ggplot(aes(factor(Weekend), TAXI_OUT)) +  geom_violin()
flights %>% filter(DAY_OF_WEEK <= 5) %>% mutate("Weekday" = 1) %>%
ggplot(aes(factor(Weekday), TAXI_OUT)) +  geom_violin() +
geom_violin(draw_quantiles=c(.25, .5, .75), scale="count")
flights %>% filter(DAY_OF_WEEK <= 5) %>% mutate("Weekday" = 1) %>%
ggplot(aes(factor(Weekday), TAXI_OUT)) +  geom_violin() +
geom_violin(draw_quantiles=c(.25, .5, .75), scale="count")
#Your client would like to understand the differences in the distributions of taxi times between weekends
#and weekdays (they don't care so much about what day it is, but just if it is a weekday or weekend).
#Further, your client is interested in seeing this across different airlines.
#Create a single visualization using a single pipeline that can do this.  As a further challenge,
#your client would like to see the airlines listed in order of the volume of flights, with the
#airline flying the most flights listed first.
#In creating your visualiztion, you should focus on clarity.  While not enough information is obviously
#not ideal, too much information can be confusing.  You should think about the balance between being
#informative about the distrubutions but sensitive to the fact that you are comparing across many distributions.
#this should be done in a single pipeline.
library(tidyverse)
flights <- read_csv('flights_jan.csv')
flights %>% mutate("Weekend" = ((flights["DAY_OF_WEEK"] > 5) = 1)) %>% ggplot(aes(factor(Weekend), TAXI_OUT)) +  geom_violin()
flights %>% filter(DAY_OF_WEEK <= 5) %>% mutate("Weekday" = 1) %>%
ggplot(aes(factor(Weekday), TAXI_OUT)) +  geom_violin() +
geom_violin(draw_quantiles=c(.25, .5, .75), scale="count")
goalies %>%
filter(year > 2000) %>%
inner_join(master_file, by=c("playerID" = "playerID")) %>%
filter(birthYear < 1974) %>%
mutate(ratioGA = SA/GA) %>%
mutate(ratioPGA = PostSA/PostGA)%>%
group_by(playerID) %>%
arrange(playerID, desc(year))%>%
#mutate(yearContinuity = lag(year)) %>%
mutate(better = ratioGA/ratioPGA) %>%
#select(playerID, year, yearContinuity, ratioGA, ratioPGA, better) %>%
mutate(continuation = (lag(better) < better)) %>%
select(playerID, year, ratioGA, ratioPGA, better, continuation, tmID) %>%
filter(continuation == TRUE) %>%
distinct(playerID, tmID) %>%
inner_join(teamvteam, by=c("tmID"="tmID")) %>%
filter(year > 2000) %>%
filter(oppID == "PIT", W >=3) %>%
distinct(playerID, tmID, year) %>%
left_join(coaches, by= c("year"="year", "tmID" = "tmID")) %>%
distinct(tmID, year, coachID) %>%
left_join(master_file, by=c("coachID"="coachID", "playerID"="playerID"))
goalies %>%
filter(year > 2000) %>%
inner_join(master_file, by=c("playerID" = "playerID")) %>%
filter(birthYear < 1974) %>%
mutate(ratioGA = SA/GA) %>%
mutate(ratioPGA = PostSA/PostGA)%>%
group_by(playerID) %>%
arrange(playerID, desc(year))%>%
#mutate(yearContinuity = lag(year)) %>%
mutate(better = ratioGA/ratioPGA) %>%
#select(playerID, year, yearContinuity, ratioGA, ratioPGA, better) %>%
mutate(continuation = (lag(better) < better)) %>%
select(playerID, year, ratioGA, ratioPGA, better, continuation, tmID) %>%
filter(continuation == TRUE) %>%
distinct(playerID, tmID) %>%
inner_join(teamvteam, by=c("tmID"="tmID")) %>%
filter(year > 2000) %>%
filter(oppID == "PIT", W >=3) %>%
distinct(playerID, tmID, year) %>%
left_join(coaches, by= c("year"="year", "tmID" = "tmID")) %>%
distinct(tmID, year, coachID) %>%
left_join(master_file, by=c("coachID"="coachID", "playerID"="playerID"))
#please only load the tidyverse library
library(tidyverse)
master_file <- read_csv('Master.csv')
goalies <- read_csv('Goalies.csv')
coaches <- read_csv('Coaches.csv')
teamvteam <- read_csv('TeamVsTeam.csv')
#please only load the tidyverse library
library(tidyverse)
#please read in the files as below.  Your code must
#run and produce the result.  If your code does not
#run on the TAs computer your HW solution will not
#be accepted.
setwd("./")
master_file <- read_csv('Master.csv')
library(tidyverse)
library(kknn)
setwd("/Users/maxryoo/Documents/Fall 2019/STAT 5630/Code/hw1")
full_data <- data.frame(read.table('Q1.txt', header=TRUE))
set.seed(1)
#Split the training and testing data evenly
training_data <- full_data[1:50, ]
testing_data <- full_data[51:100,]
train.x = training_data$X
train.y = training_data$Y
test.x = testing_data$X
test.y = testing_data$Y
test.y = test.y[order(test.x)]
test.x = test.x[order(test.x)]
training_error = replicate(21,0)
testing_error = replicate(21,0)
# 1-nearest neighbour --- you can try different k values and see the results
k = 1
knn.fit.test = kknn(y ~ x,
train = data.frame(x = train.x , y = train.y),
test = data.frame(x = test.x, y = test.y),
k = k,
kernel = "rectangular")
knn.fit.train = kknn(y ~ x,
train = data.frame(x = train.x , y =train.y ),
test = data.frame(x = train.x , y =  train.y),
k = k,
kernel = "rectangular")
test.pred = knn.fit.test$fitted.values
train.pred = knn.fit.train$fitted.values
par(mar=rep(2,4))
plot(train.x,train.y, xlim = c(1, 3), pch = 19, cex = 1, axes=FALSE, ylim = c(-4.25, 20))
title(main=paste(1, "-Nearest Neighbor Regression", sep = ""))
lines(test.x, test.x**3, col = "deepskyblue", lwd = 3)
lines(test.x, test.pred, type = "s", col = "darkorange", lwd = 3)
box()
for (i in 1:20) {
k = i
knn.fit.test = kknn(y ~ x,
train = data.frame(x = train.x , y = train.y),
test = data.frame(x = test.x, y = test.y),
k = k,
kernel = "rectangular")
knn.fit.train = kknn(y ~ x,
train = data.frame(x = train.x , y =train.y ),
test = data.frame(x = train.x , y = train.y),
k = k,
kernel = "rectangular")
test.pred = knn.fit.test$fitted.values
train.pred = knn.fit.train$fitted.values
par(mar=rep(2,4))
plot(train.x,train.y, xlim = c(1, 3), pch = 19, cex = 1, axes=FALSE, ylim = c(-4.25, 20))
title(main=paste(1, "-Nearest Neighbor Regression", sep = ""))
lines(test.x, test.x**3, col = "deepskyblue", lwd = 3)
lines(test.x, test.pred, type = "s", col = "darkorange", lwd = 3)
box()
training.error <- mean((train.pred - train.y)^2)
testing.error <- mean((test.pred - test.y)^2)
print(training.error)
print(testing.error)
training_error[i] = training.error
testing_error[i] = testing.error
}
k = 50
knn.fit.test = kknn(y ~ x,
train = data.frame(x = train.x , y = train.y ),
test = data.frame(x = test.x, y = test.y),
k = k,
kernel = "rectangular")
knn.fit.train = kknn(y ~ x,
train = data.frame(x = train.x , y = train.y ),
test = data.frame(x = train.x , y = train.y),
k = k,
kernel = "rectangular")
test.pred = knn.fit.test$fitted.values
train.pred = knn.fit.train$fitted.values
knn.fit.test = kknn(y ~ x,
train = data.frame(x = train.x , y = train.y ),
test = data.frame(x = test.x, y = test.y),
k = k,
kernel = "rectangular")
knn.fit.train = kknn(y ~ x,
train = data.frame(x = train.x , y = train.y ),
test = data.frame(x = train.x , y = train.y),
k = k,
kernel = "rectangular")
knn.fit.test$fitted.values
knn.fit.train$fitted.values
test.pred = knn.fit.test$fitted.values
train.pred = knn.fit.train$fitted.values
par(mar=rep(2,4))
plot(train.x,train.y, xlim = c(1, 3), pch = 19, cex = 1, axes=FALSE, ylim = c(-4.25, 20))
title(main=paste(1, "-Nearest Neighbor Regression", sep = ""))
lines(test.x, test.x**3, col = "deepskyblue", lwd = 3)
lines(test.x, test.pred, type = "s", col = "darkorange", lwd = 3)
box()
training.error <- mean((train.pred - train.y)^2)
testing.error <- mean((test.pred - test.y)^2)
print(training.error)
print(testing.error)
training_error[21] = training.error
testing_error[21] = testing.error
print(training_error)
print(testing_error)
which.min(testing_error)
min(testing_error)
## Plot the errors
errordf = data.frame("K" = c(1:20, 50), "Training" = training_error[1:21] , "Testing" = testing_error[1:21])
ggplot() +
geom_point(aes(x=errordf$K, y=errordf$Training,colour="darkblue")) +
geom_point(aes(x=errordf$K, y=errordf$Testing,colour="red")) +
geom_line(aes(x=errordf$K, y=errordf$Training,colour="darkblue")) +
geom_line(aes(x=errordf$K, y=errordf$Testing,,colour="red")) +
ylab("Error") +
xlab("K-Value") +
ggtitle("Testing and Training Error for K-Value 1-20 & 50") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_color_discrete(name = "Errors", labels = c("Testing", "Training"))
linreg_train <- lm(Y~X, data = training_data)
linreg_test_pred <- predict(linreg_train, newdata = testing_data)
training_summary <- summary(linreg_train)
mse_train <- mean((training_summary$residuals)^2)
mse_testing <- mean((testing_data[,2]-linreg_test_pred)^2)
### QUESTION 3 #####
nfold = 5
K=20
infold = sample(rep(1:nfold, length.out=length(train.x)))
mydata = data.frame(x = train.x, y = train.y)
errorMatrix.train = matrix(NA, K + 1, nfold)
errorMatrix.test = matrix(NA, K + 1, nfold)
for (l in 1:nfold) {
for (k in 1:K) {
knn.fit.train.val = kknn(y ~ x, train = mydata[infold != l, ], test = mydata[infold != l, ], k = k)
errorMatrix.train[k, l] = mean((knn.fit.train.val$fitted.values - mydata$y[infold != l])^2)
knn.fit.test.val = kknn(y ~ x, train = mydata[infold != l, ], test = mydata[infold == l, ], k = k)
errorMatrix.test[k, l] = mean((knn.fit.test.val$fitted.values - mydata$y[infold == l])^2)
}
}
K = 50 # maximum number of k that I am considering
errorMatrix = matrix(NA, K, nfold) # save the prediction error of each fold
for (l in 1:nfold)
for (l in 1:nfold) {
for (k in 1:K) {
knn.fit = kknn(y ~ x, train = mydata[infold != l, ], test = mydata[infold == l, ], k = k)
errorMatrix[k, l] = mean((knn.fit$fitted.values - mydata$y[infold == l])^2)
}
}
for (l in 1:nfold) {
for (k in 1:K) {
knn.fit = kknn(y ~ x, train = mydata[infold != l, ], test = mydata[infold == l, ], k = k)
errorMatrix[k, l] = mean((knn.fit$fitted.values - mydata$y[infold == l])^2)
}
}
for(l in 1:50) {
knn.fit.train.val = kknn(y ~ x, train = mydata[infold != l, ], test = mydata[infold != l, ], k = 50)
print(mean((knn.fit.train.val$fitted.values - mydata$y[infold != l])^2))
knn.fit.test.val = kknn(y ~ x, train = mydata[infold != l, ], test = mydata[infold == l, ], k = 50)
print(mean((knn.fit.test.val$fitted.values - mydata$y[infold == l])^2))
}
