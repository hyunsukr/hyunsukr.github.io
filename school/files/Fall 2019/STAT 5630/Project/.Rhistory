sum(boot_difference[[x]] >=
(mean(pop1mc[,x])-mean(pop2mc[,x]))))/10000
sum(p <= 0.05)/100
pop1mc <- replicate(100, sample(data4$pop1, 9))
pop2mc <- replicate(100, sample(data4$pop2, 9))
boot_pop12mc <- lapply(1:100, function(x)
replicate(10000, sample(c(pop1mc[,x],pop2mc[,x]),
replace=T)))
boot_difference <- lapply(boot_pop12mc,
function(x)
apply(x[1:9,],2,mean) -
apply(x[10:18,],2,mean))
p <- sapply(1:100, function(x)
sum(boot_difference[[x]] >=
(mean(pop1mc[,x])-mean(pop2mc[,x]))))/10000
sum(p <= 0.05)/100
pop1mc <- replicate(100, sample(data4$pop1, 9))
pop2mc <- replicate(100, sample(data4$pop2, 9))
boot_pop12mc <- lapply(1:100, function(x)
replicate(10000, sample(c(pop1mc[,x],pop2mc[,x]),
replace=T)))
boot_difference <- lapply(boot_pop12mc,
function(x)
apply(x[1:9,],2,mean) -
apply(x[10:18,],2,mean))
p <- sapply(1:100, function(x)
sum(boot_difference[[x]] >=
(mean(pop1mc[,x])-mean(pop2mc[,x]))))/10000
sum(p <= 0.05)/100
pop1mc <- replicate(100, sample(data4$pop1, 9))
pop2mc <- replicate(100, sample(data4$pop2, 9))
boot_pop12mc <- lapply(1:100, function(x)
replicate(10000, sample(c(pop1mc[,x],pop2mc[,x]),
replace=T)))
boot_difference <- lapply(boot_pop12mc,
function(x)
apply(x[1:9,],2,mean) -
apply(x[10:18,],2,mean))
p <- sapply(1:100, function(x)
sum(boot_difference[[x]] >=
(mean(pop1mc[,x])-mean(pop2mc[,x]))))/10000
sum(p <= 0.05)/100
print(power)
power <- sum(p <= 0.05)/100
print(power)
print(power1)
power1 <- sum(p <= 0.05)/100
print(power1)
rep_samps <- replicate(10000, rnorm(27, mean=62.9, sd=13.3))
rep_means <- apply(rep_samps2,2,mean)
rep_sd <- apply(rep_samps2,2,function(x)sd(x))
t_scores <- (rep_means2-62)/(rep_sd2/sqrt(27))
t_rejection <- sapply(t_scores2,function(x)>=qt(.95, df=27))
rep_samps <- replicate(10000, rnorm(27, mean=62.9, sd=13.3))
rep_means <- apply(rep_samps2,2,mean)
rep_sd <- apply(rep_samps2,2,function(x)sd(x))
t_scores <- (rep_means2-62)/(rep_sd2/sqrt(27))
t_rejection <- sapply(t_scores2,function(x)(x)>=qt(.95, df=27))
reject_2 <- sum(t_rejection)
print(reject_2/10000)
efs <- (62.9-62)/13.3
print(efs)
rep_samps2 <- replicate(10000,
rnorm(1352,
mean=62.9,
sd=13.3))
rep_means2 <- apply(rep_samps2,2,mean)
rep_sd2 <- apply(rep_samps2,2,function(x)sd(x))
t_scores2 <- (rep_means2-62)/(rep_sd2/sqrt(1352))
t_rejection2 <- sapply(t_scores2,
function(x)(x)>=qt(.95,
df=1352))
reject_2 <- sum(t_rejection2)
print(reject_2/10000)
rep_samps <- replicate(10000, rnorm(27, mean=62.9, sd=13.3))
rep_means <- apply(rep_samps,2,mean)
rep_sd <- apply(rep_samps,2,function(x)sd(x))
t_scores <- (rep_means-62)/(rep_sd/sqrt(27))
t_rejection <- sapply(t_scores,function(x)(x)>=
qt(.95, df=27))
reject <- sum(t_rejection)
print(reject/10000)
## Register data
rpA <- c(5.8, 1, 1.1, 2.1, 2.5, 1.1, 1, 1.2, 3.2, 2.7)
rpB <- c(1.5, 2.7, 6.6, 4.6, 1.1, 1.2, 5.7, 3.2, 1.2, 1.3)
## Conduct the test
wilcox.test(rpA, rpB, alternative="two.sided")
wilcox.test(rpA, rpB, alternative="two.sided", exact=FALSE)
wilcox.test(rpA, rpB, alternative="two.sided", exact=FALSE, correct=FALSE)
## Test statistic calculation
rpA.data <- data.frame(time=rpA, person="A")
rpB.data <- data.frame(time=rpB, person="B")
rp.data <- rbind(rpA.data, rpB.data)
rp.data$rank <- rank(rp.data$time)
rp.data
sums <- tapply(rp.data$rank, rp.data$person, sum)
sums
nA <- length(rpA)
nB <- length(rpB)
min(tsA,tsB)
tsA <- nA*nB + (nA*(nA+1)/2) - sums[1]
tsB <- nA*nB + (nB*(nB+1)/2) - sums[2]
min(tsA,tsB)
## Create two-way table for 114 individuals' hair and eye color
M <- matrix(c(38,14,11,51), nrow=2, ncol=2)
dimnames(M) <- list(Hair=c("Fair","Dark"), Eye=c("Blue","Brown"))
M
## Create a two-way table for 400 individuals' gender and education
L <- matrix(c(63,42,54,44,46,53,41,57), nrow=2, ncol=4)
dimnames(L) <- list(Gender=c("Female","Male"),
Education=c("HS","BA","MA","PhD"))
L
sum(L)
## Joint distribution in proportions
Lp <- prop.table(L)
sum(Lp)
## Conditional distributions of education levels
L.cond.edu <- prop.table(L,2)
L.cond.edu
apply(L.cond.edu,2,sum)
apply(L.cond.edu,2,sum)
L.cond.edu
## Joint distribution in proportions
Lp <- prop.table(L)
sum(Lp)
Lp
### IN CLAS ###
library(ggplot2)
k <- 10000
n <-7
ps <- 0.3
mu <- n * ps
ggplot(df, aes(x=sampes)) + geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") + stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
k <- 10000
n <-7
ps <- 0.3
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
ggplot(df, aes(x=sampes)) + geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") + stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
k <- 10000
n <-7
ps <- 0.3
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, probs=ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-28
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-49
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-70
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-28
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-49
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
k <- 10000
n <-49
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
n <-70
ps <- 0.1
mu <- n * ps
sigma <- sqrt(n*ps*(1-ps))
df <- data.frame(samps=rbinom(n=k, size=n, prob=ps))
ggplot(df, aes(x=samps)) +
geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
stat_function(fun=dnorm, args=list(mean=mu, sd=sigma))
## Conduct the test
phat <- c(0.117, 0.121)
n <- c(50000, 60000)
x <- phat*n
z2.prop <- prop.test(x, n, alternative="two.sided", correct=FALSE)
z2.prop
## Determine test statistic
comb.phat <- sum(x)/sum(n)
z <- diff(phat)/sqrt(comb.phat*(1-comb.phat)*(1/n[1]+1/n[2]))
z
## Determine p-value
p.value <- 2*(1-pnorm(z))
p.value
## Compare test statistic and p-value to output
sqrt(z2.prop$statistic)
z2.prop$p.value
z1.prop <- prop.test(x, n, alternative="one.sided", correct=FALSE)
z1.prop <- prop.test(x, n, alternative="less", correct=FALSE)
z1.prop$p.value
## Calculate the power
library(pwr)
pwr.chisq.test(w=0.3, N=400, df=3, sig.level=0.05)
## Determine the sample size required for power of 0.8
pwr.chisq.test(w=0.3, df=3, sig.level=0.05, power=0.8)
## Calculate the power
pwr.chisq.test(w=0.1, N=200, df=3, sig.level=0.05)
## Determine the sample size required for power of 0.8
pwr.chisq.test(w=0.1, df=3, sig.level=0.05, power=0.8)
## Calculate the power
pwr.chisq.test(w=0.1, N=200, df=3, sig.level=0.05)
## Determine the sample size required for power of 0.8
pwr.chisq.test(w=0.1, df=3, sig.level=0.05, power=0.8)
## Calculate the power
pwr.p.test(h=-0.2, n=100, sig.level=0.05, alternative="less")
## Determine the sample size required for power of 0.8
pwr.p.test(h=-0.2, sig.level=0.05, power=0.8, alternative="less")
#starting to look at pipelines
library(tidyverse)
salary_games_info <- read_csv('Lect2data.csv')
head(salary_games_info)
##############################################################
##############################################################
################### Ceci n'est pas une pipe ##################
##############################################################
##############################################################
salary_games_info %>% head()
salary_games_info %>% head() %>% tail(3)
tail(head(salary_games_info), 3)
salary_small_save <- salary_games_info %>% head() %>% tail(3)
##############################################################
##############################################################
######################## SUMMARIZE ###########################
##############################################################
##############################################################
salary_games_info %>%
summarize(count=n())
salary_games_info %>%
summarize(average_salary=mean(salary))
salary_games_info %>% summary()
salary_games_info %>%
summarize(average_salary=mean(salary)) %>% is_tibble()
salary_games_info %>% summary() %>% is_tibble()
##############################################################
##############################################################
########################## FILTER ############################
##############################################################
##############################################################
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000)
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000) %>% tail()
##############################################################
##############################################################
######################### ARRANGE ############################
##############################################################
##############################################################
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000) %>% arrange(desc(yearID))
salary_games_info %>% filter(yearID > 1986 & yearID <= 2000) %>% arrange(desc(yearID), salary)
##############################################################
##############################################################
######################## GROUP_BY ############################
##############################################################
##############################################################
salary_games_info %>%
group_by(playerID) %>%
summarize(count=n())
salary_games_info %>%
group_by(playerID) %>%
summarize(count=n()) %>%
arrange(desc(count))
salary_games_info %>%
group_by(playerID) %>%
summarize(numYearsPlayed = length(unique(yearID)), count=n())
salary_games_info %>%
group_by(playerID) %>%
arrange(yearID) %>%
mutate(yearContinuity = yearID - lag(yearID)) %>%
filter(yearID>1985)
##############################################################
##############################################################
######################### MUTATE #############################
##############################################################
##############################################################
salary_games_info %>% mutate(sqrt_games = sqrt(G))
salary_games_info %>% mutate(atbats_per_game = AB/G)
#starting to look at pipelines
library(tidyverse)
package.install(tidyverse)
install.packages(tidyverse)
install.packages(tidyverse)
install.package(tidyverse)
# goal is to get data into tidy format.
# often data is not tidy, so you have to do some work
# what is tidy data?
#     each variable has its own column
#     each observation gets its own row, and not multiple rows
#     one datapoint per cell
library(tidyverse)
data <- tibble(group = sample(c('red', 'green', 'blue'), 100, replace=TRUE),
`2011`=runif(100), `2012`=rnorm(100), `2013`=1:100)
data.tidy <- data %>% gather(key="year", value="measurement", -group)
data.tidy <- data %>% gather(key="year", value="measurement", -group) %>%
mutate(year=as.integer(year))
data <- tibble(observation=rep(1:10, each=3),
meas_var=rep(c('length', 'width', 'height'), times=10),
measurement=runif(30))
data.tidy <- data %>% spread(key="meas_var", value="measurement")
data.tidy
data <- tibble(group = sample(c('red', 'green', 'blue'), 100, replace=TRUE),
`2011`=runif(100), `2012`=rnorm(100), `2013`=1:100)
data
data.tidy <- data %>% gather(key="year", value="measurement", -group)
data.tidy
data.tidy <- data %>% gather(key="year", value="measurement", -group) %>%
mutate(year=as.integer(year))
data.tidy
View(data.tidy)
data <- tibble(observation=rep(1:10, each=3),
meas_var=rep(c('length', 'width', 'height'), times=10),
measurement=runif(30))
view(data)
View(data)
View(data)
data.tidy
View(data)
View(data.tidy)
data.tidy <- data %>% spread(key="meas_var", value="measurement")
data.tidy <- data %>% spread(key="meas_var", value="measurement")
View(data.tidy)
View(data)
library(tidyverse)
flights <- read_csv('flights_jan.csv')
flights %>% mutate("Weekend" = ((flights["DAY_OF_WEEK"] > 5) = 1)) %>% ggplot(aes(factor(Weekend), TAXI_OUT)) +  geom_violin()
flights %>% filter(DAY_OF_WEEK <= 5) %>% mutate("Weekday" = 1) %>%
ggplot(aes(factor(Weekday), TAXI_OUT)) +  geom_violin() +
geom_violin(draw_quantiles=c(.25, .5, .75), scale="count")
flights %>% filter(DAY_OF_WEEK <= 5) %>% mutate("Weekday" = 1) %>%
ggplot(aes(factor(Weekday), TAXI_OUT)) +  geom_violin() +
geom_violin(draw_quantiles=c(.25, .5, .75), scale="count")
#Your client would like to understand the differences in the distributions of taxi times between weekends
#and weekdays (they don't care so much about what day it is, but just if it is a weekday or weekend).
#Further, your client is interested in seeing this across different airlines.
#Create a single visualization using a single pipeline that can do this.  As a further challenge,
#your client would like to see the airlines listed in order of the volume of flights, with the
#airline flying the most flights listed first.
#In creating your visualiztion, you should focus on clarity.  While not enough information is obviously
#not ideal, too much information can be confusing.  You should think about the balance between being
#informative about the distrubutions but sensitive to the fact that you are comparing across many distributions.
#this should be done in a single pipeline.
library(tidyverse)
flights <- read_csv('flights_jan.csv')
flights %>% mutate("Weekend" = ((flights["DAY_OF_WEEK"] > 5) = 1)) %>% ggplot(aes(factor(Weekend), TAXI_OUT)) +  geom_violin()
flights %>% filter(DAY_OF_WEEK <= 5) %>% mutate("Weekday" = 1) %>%
ggplot(aes(factor(Weekday), TAXI_OUT)) +  geom_violin() +
geom_violin(draw_quantiles=c(.25, .5, .75), scale="count")
goalies %>%
filter(year > 2000) %>%
inner_join(master_file, by=c("playerID" = "playerID")) %>%
filter(birthYear < 1974) %>%
mutate(ratioGA = SA/GA) %>%
mutate(ratioPGA = PostSA/PostGA)%>%
group_by(playerID) %>%
arrange(playerID, desc(year))%>%
#mutate(yearContinuity = lag(year)) %>%
mutate(better = ratioGA/ratioPGA) %>%
#select(playerID, year, yearContinuity, ratioGA, ratioPGA, better) %>%
mutate(continuation = (lag(better) < better)) %>%
select(playerID, year, ratioGA, ratioPGA, better, continuation, tmID) %>%
filter(continuation == TRUE) %>%
distinct(playerID, tmID) %>%
inner_join(teamvteam, by=c("tmID"="tmID")) %>%
filter(year > 2000) %>%
filter(oppID == "PIT", W >=3) %>%
distinct(playerID, tmID, year) %>%
left_join(coaches, by= c("year"="year", "tmID" = "tmID")) %>%
distinct(tmID, year, coachID) %>%
left_join(master_file, by=c("coachID"="coachID", "playerID"="playerID"))
goalies %>%
filter(year > 2000) %>%
inner_join(master_file, by=c("playerID" = "playerID")) %>%
filter(birthYear < 1974) %>%
mutate(ratioGA = SA/GA) %>%
mutate(ratioPGA = PostSA/PostGA)%>%
group_by(playerID) %>%
arrange(playerID, desc(year))%>%
#mutate(yearContinuity = lag(year)) %>%
mutate(better = ratioGA/ratioPGA) %>%
#select(playerID, year, yearContinuity, ratioGA, ratioPGA, better) %>%
mutate(continuation = (lag(better) < better)) %>%
select(playerID, year, ratioGA, ratioPGA, better, continuation, tmID) %>%
filter(continuation == TRUE) %>%
distinct(playerID, tmID) %>%
inner_join(teamvteam, by=c("tmID"="tmID")) %>%
filter(year > 2000) %>%
filter(oppID == "PIT", W >=3) %>%
distinct(playerID, tmID, year) %>%
left_join(coaches, by= c("year"="year", "tmID" = "tmID")) %>%
distinct(tmID, year, coachID) %>%
left_join(master_file, by=c("coachID"="coachID", "playerID"="playerID"))
#please only load the tidyverse library
library(tidyverse)
master_file <- read_csv('Master.csv')
goalies <- read_csv('Goalies.csv')
coaches <- read_csv('Coaches.csv')
teamvteam <- read_csv('TeamVsTeam.csv')
#please only load the tidyverse library
library(tidyverse)
#please read in the files as below.  Your code must
#run and produce the result.  If your code does not
#run on the TAs computer your HW solution will not
#be accepted.
setwd("./")
master_file <- read_csv('Master.csv')
data_frame<-data.frame(data)
# setting data for testing and training
set.seed(1)
#### Easy
filename_news <- "/Users/maxryoo/Documents/Fall 2019/Research/Simulation/news/news_text.txt"
file_info_news <- readChar(filename_news, file.info(filename_news)$size)
setwd("/Users/maxryoo/Documents/Fall 2019/Research/")
#### Easy
filename_news <- "/Users/maxryoo/Documents/Fall 2019/Research/Simulation/news/news_text.txt"
file_info_news <- readChar(filename_news, file.info(filename_news)$size)
file_info_news <- gsub("\r", "",file_info_news)
file_info_news <- gsub("\n", "",file_info_news)
short_easy_252 <- substr(file_info_news, 1, 252)
middle_easy_504 <- substr(file_info_news, 1, 504)
length_easy_1007 <- substr(file_info_news, 1, 1007)
short_easy_252
middle_easy_504 <- substr(file_info_news, 1, 504)
middle_easy_504
short_easy_252
middle_easy_504
length_easy_1007 <- substr(file_info_news, 1, 1007)
length_easy_1007
#####
# Final Project
#
# import data from file
setwd("/Users/maxryoo/Documents/Fall 2019/STAT 5630/Project")
opioids = read.csv("data/opioids.csv")
info = read.csv("data/prescriber-info.csv")
class = read.csv("data/drugclass.csv")
overdose = read.csv("data/overdoses.csv")
colnames(class)[1] = "Drug.Name"
# add drug class to the info matrix
classunique=unique(class$Drug.Class)
info.drug = colnames(info[,6:255])
classmask = data.frame(row.names=classunique)
for (c in 1 : length(classunique)) {
r = info[1,6:255]
rownames(r) = classunique[c]
for (k in 1:length(info.drug)) {
if (info.drug[k] == class$Drug.Name[k] && class$Drug.Class[k] == classunique[c]){
r[k] = 1
} else {
r[k] = 0
}
}
classmask = rbind(classmask,r)
}
# Apply mask
info.all = cbind(info,data.frame(t(as.matrix(classmask) %*% as.matrix(t(info[,6:255])))))
info.class = cbind(info.all[,1:5], info.all[,256:318])
head(info.all)
nofull = overdose[,2:4]
colnames(nofull) <- c("Population", "Deaths", "State")
info.full <- merge(info.class, nofull)
info.full$deathrate <- as.integer(info.full$Deaths) / as.integer(info.full$Population)
head(info.full)
#
# docop=info[info$Opioid.Prescriber==1,]
info.full
install.packages("prcomp")
install.packages("prcomp")
